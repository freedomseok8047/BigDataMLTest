{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 마이닝\n",
    "#### 감성 분석, 토픽 분석\n",
    "#### 샘플 데이터, 네이버 크롤러로 수집된, 뉴스기사, 영화 리뷰 데이터\n",
    "#### \n",
    "#### 용어정리 \n",
    "#### 텍스트 마이닝\n",
    "#### 비정형 텍스트 데이터 패턴 찾아 내고 의미있는 정보를 추출 및 분석 과정\n",
    "#### 과정 : 텍트 전처리 -> 특성 벡터화 -> 머신러닝 모델 구축 및 학습 / 평가 -> 시각화 등 \n",
    "\n",
    "#### 특성 백터화와 특성 추출\n",
    "#### 단어 기반 특성 추출 , 숫자 형 값인 벡터 값으로 표현\n",
    "#### 특성 벡터화 종류 -> 1) BoW (Bag of Words = 말 뭉치)와 Word2vec\n",
    "#### 1) BoW -> 문서가 가지고 있는 모든 단어에 대해 순서는 무시하고, 빈도만 고려해서 \n",
    "#### 해당 단어가 얼마나 자주 등장하는지 특성 벡터를 만드는 방법\n",
    "#### 종류) a) 카운트 기반 벡터 b) TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터 방식\n",
    "#### a) 카운트 기반 벡터\n",
    "#### 단어 피처에 숫자형 값을 할당 할시, 각 문서에서 해당 단어가 등장하는 횟수,\n",
    "#### 단어의 빈도를 부여하는 벡터화 방식 (숫자형으로 변경한다.)\n",
    "#### 예) 전체 문서에 등장한 단어을 기반으로 어휘 사전을 생성하고 , 단어마다 등장 횟수를 카운트하여 \n",
    "#### 해당 단어의 정수 벡터 값으로 할당한다.\n",
    "#### 문서별 단어 빈도를 정리하는 ,행렬 -> 문서 단어 행렬 (DTM Document Term Matrix)\n",
    "#### 단어 출현 빈도가 높을 수록 중요한 단어로 취급한다.\n",
    "#### 문서 d에 등장하는 단어 t의 횟수 -> tf(t,d)로 표기\n",
    "#### 단어 행렬: 예) 문서1 -> 사과:10 , 점심 : 30  -> tf(\"점심\", 문서1) = 30\n",
    "#### 사이킷 런에서 -> CountVectorizer 모듈(파일)에서 제공함\n",
    "\n",
    "#### b) TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터 방식\n",
    "#### 카운트 기반에서, 단순 해당 단어빈도가 많으면 그냥 중요하다고 단순 판단\n",
    "#### 해당 단어가 단지 문장 구성상 많ㅇ ㅣ사용하는 단어 일 가능성 일수 있다.\n",
    "#### TF-IDF는 \n",
    "#### 특정 문서에 많이 나타나는 단어는 해당 문서 단어 벡터에 가중치를 높이고\n",
    "#### 모든 문서에 많이 나타나는 단어는 문서의 특징을 나타내는 단어가 아니고,\n",
    "#### 그냥 범용적으로 사용 되는 단어 가중치를 낮추는 방식\n",
    "\n",
    "#### 문서 d에 등장한 단어 t의 TF-IDF 표현식\n",
    "#### tf - idf(t,d) = tf(t,d) * idf(t,d)\n",
    "#### tf(t,d) : 문서 d에 등장하는 단어 t의 횟수 \n",
    "#### idf(t,d) : 역문서 빈도\n",
    "\n",
    "#### 집계 형식 : 문서1 : 사과 : 0.2 , 점심 : 0.6 , tf-idf(\"점심\", 문서1) = 0.6\n",
    "#### 사이킷 런 TfidfVectourizer 제공함\n",
    "\n",
    "#### 감성분석\n",
    "#### 텍트에서 사용자의 주관적인 의견, 감성, 태도를 분석하는 테그스트 마이닝 핵심 분석 기법 중 하나, \n",
    "#### 오피니언 마이닝이라고도 부름\n",
    "#### 감섬을 나타내는 단어를 기반으로 긍정 또는 부정인지 결정을 함 \n",
    "#### 감성 사전 기빈의 감성 분석을 함 \n",
    "#### 해당 감성 사전을 가진 상태에서 단어를 검색해서 점수를 계산함 \n",
    "\n",
    "#### 토픽 모델링\n",
    "#### 문서를 구성하는 키워드를 기반으로 토픽(주제)를 추출하고, 추출한 토픽을 기준으로 문서를 분류\n",
    "#### 분류(클러스터링) 및 분석. 토픽을 도출하고, 동향을 파악해서, 새로운 문서의 토픽을 예측하는 기법(분석)\n",
    "\n",
    "#### 토픽 모델링을 하는 분석 방법 중 하나 -> LDA (Latent Dirichlet Allocation)\n",
    "#### 주어진 문서에 잠재된 토픽을 추론하는 확율 모델 알고리즘\n",
    "#### 하나의 문서에 여러 토픽으로 구성되어 있고, 문서의 토픽의 분포에 따라서 단어의 분포가 결정됨 \n",
    "#### 하나의 문서에 잠재된 토픽별 단어와 관련된 토픽 별 비율을 도출하는 방법\n",
    "#### 프로세스 \n",
    "#### 디리클레 대개변수 -> 문서 d의 토픽비율 -> 문서 d의 단어 n에서 토픽할당 -> 문서 d의 단어 n\n",
    "#### 토픽 하이퍼 매개변수 -> 토픽 k의 단어 분포 생성 확률 -> 문서 d의 단어 n\n",
    "\n",
    "#### pyLDAvis, 시각화 도구\n",
    "#### 유사성에 따라 토픽간 거리 지도와 선택한 토픽에 관련성 높은 단어 30개를 비차트로 시각화 해주는 도구 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
